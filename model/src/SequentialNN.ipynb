{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917f1aae-7778-49e0-9098-67d0f44c32a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import LabelEncoder, LabelBinarizer\n",
    "from sklearn.metrics import r2_score, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc, RocCurveDisplay\n",
    "from sklearn.utils import compute_class_weight\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import tensorflow_model_optimization as tfmot\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import joblib\n",
    "import pathlib\n",
    "import os\n",
    "from data_parser import parse_json_data, process_path, parse_label\n",
    "\n",
    "tf.config.run_functions_eagerly(True)\n",
    "tf.data.experimental.enable_debug_mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627ba094-2b00-492b-9000-41a78d19c9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testdir\n",
    "path = os.getcwd()\n",
    "print(\"PATH: \", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7f95a0-9b6e-4c55-81d9-349e15cd5802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset\n",
    "all_features = []\n",
    "all_labels = []\n",
    "data_path = \"../data/\"\n",
    "\n",
    "pes_features = []\n",
    "wool_features = []\n",
    "cotton_features = []\n",
    "\n",
    "files = process_path(data_path)\n",
    "for file_path in files:\n",
    "    file_label = parse_label(file_path)\n",
    "    file_features = parse_json_data(file_path)\n",
    "    if (file_label == \"pes\"):\n",
    "        pes_features.append(file_features)\n",
    "    elif (file_label == \"villa\"):\n",
    "        wool_features.append(file_features)\n",
    "    elif (file_label == \"puuvilla\"):\n",
    "        cotton_features.append(file_features)\n",
    "    all_labels.append(file_label)\n",
    "    all_features.append(file_features)\n",
    "\n",
    "# Define test size and validation size split\n",
    "test_size_split = 0.20\n",
    "\n",
    "validation_size_split = 0.20\n",
    "\n",
    "if len(pes_features) > len(cotton_features) & len(pes_features) > len(wool_features):\n",
    "    max_size = len(pes_features)\n",
    "elif len(cotton_features) > len(pes_features) & len(cotton_features) > len(wool_features):\n",
    "    max_size = len(cotton_features)\n",
    "else:\n",
    "    max_size = len(wool_features)\n",
    "print(f\"Max Size for equal distribution {max_size}\")\n",
    "# Here we split dataset to equal distribution where rounded to nearest 10 scans / sample\n",
    "# and where length of the training set can be divisible to nearest 10 scans for test split\n",
    "feature_samples_count = max_size - (int)(max_size*validation_size_split)\n",
    "print(feature_samples_count)\n",
    "while feature_samples_count % 10 != 0:\n",
    "    feature_samples_count -= 1\n",
    "\n",
    "print(\"Nearest 10 scans count for all features after validation split\")\n",
    "print(feature_samples_count)\n",
    "\n",
    "validation_samples_count = max_size - feature_samples_count\n",
    "print(validation_samples_count)\n",
    "while validation_samples_count % 10 != 0:\n",
    "    validation_samples_count -= 1\n",
    "\n",
    "print(\"Nearest 10 scans count for validation split\")\n",
    "print(validation_samples_count)\n",
    "\n",
    "pes = pd.DataFrame(pes_features[:feature_samples_count])\n",
    "cotton = pd.DataFrame(cotton_features[:feature_samples_count])\n",
    "wool = pd.DataFrame(wool_features[:feature_samples_count])\n",
    "# Combine features\n",
    "all_eq_features = pd.concat([pes, cotton, wool], axis=0).reset_index(drop=True)\n",
    "\n",
    "last = feature_samples_count + validation_samples_count\n",
    "pes_val = pd.DataFrame(pes_features[feature_samples_count:last])\n",
    "cotton_val = pd.DataFrame(cotton_features[feature_samples_count:last])\n",
    "wool_val = pd.DataFrame(wool_features[feature_samples_count:last])\n",
    "# Combine validation dataset\n",
    "all_val_features = pd.concat([pes_val, cotton_val, wool_val], axis=0).reset_index(drop=True)\n",
    "\n",
    "# One-hot encode labels dynamically\n",
    "fabric_types = [\"pes\", \"puuvilla\", \"villa\"]\n",
    "label_array = np.array([[1, 0, 0]] * feature_samples_count \n",
    "                       + [[0, 1, 0]] * feature_samples_count \n",
    "                       + [[0, 0, 1]] * feature_samples_count\n",
    "                      )\n",
    "labels_eq = pd.DataFrame(label_array, columns=fabric_types)\n",
    "\n",
    "y_val_labels = np.array([[1, 0, 0]] * validation_samples_count \n",
    "                       + [[0, 1, 0]] * validation_samples_count\n",
    "                       + [[0, 0, 1]] * validation_samples_count\n",
    "                      )\n",
    "val_labels_eq = pd.DataFrame(y_val_labels, columns=fabric_types)\n",
    "val_labels_int = np.argmax(val_labels_eq, axis=1)\n",
    "\n",
    "# Full dataset in dataframes\n",
    "features = pd.DataFrame(all_features)\n",
    "labels = pd.DataFrame(all_labels, columns=['fabric_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697883c4-6372-43aa-b0d9-f2e440c0de24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode fabric_type labels with integers (Used to reverse labels back easily if needed)\n",
    "label_encoder = LabelEncoder()\n",
    "labels['fabric_type_encoded'] = label_encoder.fit_transform(labels['fabric_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a86c2b-74b4-4c2c-af03-77614aae4184",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_labels = labels_eq.idxmax(axis=1)\n",
    "\n",
    "# Split data to train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    all_eq_features, \n",
    "    labels_eq, \n",
    "    test_size=test_size_split, \n",
    "    random_state=42,\n",
    "    stratify=y_labels\n",
    ")\n",
    "y_train_int = np.argmax(y_train, axis=1)\n",
    "y_test_int = np.argmax(y_test, axis=1)\n",
    "\n",
    "def plot_class_distribution(y_data, label=\"Dataset\"):\n",
    "    class_counts = pd.Series(y_data).value_counts()\n",
    "    class_counts.plot(kind='bar')\n",
    "    plt.title(f\"Class Distribution in {label}\")\n",
    "    plt.xlabel(\"Class\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()\n",
    "\n",
    "# Convert one-hot encoded labels back to categorical labels\n",
    "y_train_labels = y_train.idxmax(axis=1)\n",
    "y_test_labels = y_test.idxmax(axis=1)\n",
    "\n",
    "# Plot for train and test sets\n",
    "plot_class_distribution(y_train_labels, \"Training Set\")\n",
    "plot_class_distribution(y_test_labels, \"Testing Set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdf2f1d-2ab5-4b42-81e3-0b5bec72ab8f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define total epochs\n",
    "total_epochs = 16\n",
    "k = 16 # Number of SVD components to keep\n",
    "n_pls = 14 # Number of PLS components\n",
    "\n",
    "# Step 1: Fit SVD on the training set (Feature Reduction)\n",
    "# Exports the right singular vector V in\n",
    "# A = USV^T\n",
    "svd = TruncatedSVD(n_components=k)\n",
    "X_train_svd = svd.fit_transform(X_train)  # Transform data\n",
    "X_test_svd = svd.transform(X_test)\n",
    "svd_matrix = svd.components_.T  # Transformation matrix -> AV\n",
    "\n",
    "# Step 2: Fit PLS on the reduced SVD data\n",
    "# Finds the latent variables T (scores of X) and U (scores for Y) from AV\n",
    "pls = PLSRegression(n_components=n_pls)\n",
    "X_train_pls = pls.fit_transform(X_train_svd, y_train)  # Fit on SVD-transformed features\n",
    "X_test_pls = pls.transform(X_test_svd)  # Apply transformation\n",
    "pls_matrix = pls.x_weights_  # Extract PLS transformation matrix\n",
    "\n",
    "# Reduce input data into k components\n",
    "# where the input parameters (512) -> A\n",
    "# A approximates to USV^T\n",
    "# and result becomes AV = UkSk (left singular vectors + values)\n",
    "class SVDLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, svd_matrix, **kwargs):\n",
    "        super(SVDLayer, self).__init__(**kwargs)\n",
    "        self.svd_matrix = self.add_weight(\n",
    "            name=\"svd_matrix\", \n",
    "            shape=svd_matrix.shape, \n",
    "            initializer=tf.constant_initializer(svd_matrix), \n",
    "            trainable=True,\n",
    "        )\n",
    "        self.bias = self.add_weight(\n",
    "            name=\"bias\",\n",
    "            shape=svd_matrix.shape[1],\n",
    "            initializer=\"zeros\",\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.svd_matrix) + self.bias\n",
    "\n",
    "# Projects AV Input into latent space\n",
    "# Creates a score matrix T\n",
    "# where x_weights (noted W) are used for calculating X scores (T)\n",
    "# then T = AV input * W\n",
    "# is AV components with maximum cross-covariance\n",
    "class PLSLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, pls_matrix, **kwargs):\n",
    "        super(PLSLayer, self).__init__(**kwargs)\n",
    "        self.pls_matrix = self.add_weight(\n",
    "            name=\"pls_matrix\", \n",
    "            shape=pls_matrix.shape, \n",
    "            initializer=tf.constant_initializer(pls_matrix), \n",
    "            trainable=True,\n",
    "        )\n",
    "        self.bias = self.add_weight(\n",
    "            name=\"bias\",\n",
    "            shape=pls_matrix.shape[1],\n",
    "            initializer=\"zeros\",\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.pls_matrix) + self.bias\n",
    "\n",
    "\n",
    "# Build Model\n",
    "# Project sample features into k number of right singular vector space (AV)\n",
    "# Further reduce dimensions by projecting the input layer into n_pls components\n",
    "model = keras.models.Sequential([\n",
    "    SVDLayer(svd_matrix),\n",
    "    PLSLayer(pls_matrix),\n",
    "    keras.layers.Dense(1000, activation='relu'),\n",
    "    keras.layers.Dense(300, activation='relu'),\n",
    "    keras.layers.Dense(60, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='relu'),\n",
    "    keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(\n",
    "    X_train, \n",
    "    y_train_int,\n",
    "    validation_data=(all_val_features, val_labels_int),\n",
    "    epochs=total_epochs, \n",
    ")\n",
    "\n",
    "model.evaluate(X_test, y_test_int, verbose=0)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4197f7-25f0-4b06-b3c6-f81b724564a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize how a single sample looks through the data-augment without bias\n",
    "\n",
    "pes_sample = np.expand_dims(pes_features[7], axis=0)\n",
    "cotton_sample = np.expand_dims(cotton_features[7], axis=0)\n",
    "wool_sample = np.expand_dims(wool_features[7], axis=0)\n",
    "\n",
    "\n",
    "# Apply SVD and PLS transformations\n",
    "pes_svd = tf.matmul(pes_sample, svd_matrix)\n",
    "pes_pls = tf.matmul(pes_svd, pls_matrix)\n",
    "\n",
    "cotton_svd = tf.matmul(cotton_sample, svd_matrix)\n",
    "cotton_pls = tf.matmul(cotton_svd, pls_matrix)\n",
    "\n",
    "wool_svd = tf.matmul(wool_sample, svd_matrix)\n",
    "wool_pls = tf.matmul(wool_svd, pls_matrix)\n",
    "\n",
    "# Visualize SVD results\n",
    "plt.figure(figsize=(20, 14))\n",
    "plt.title(\"SVD Output (All Materials)\")\n",
    "plt.plot(pes_svd.numpy()[0], label='PES')\n",
    "plt.plot(cotton_svd.numpy()[0], label='Cotton')\n",
    "plt.plot(wool_svd.numpy()[0], label='Wool')\n",
    "plt.xlabel(\"SVD Component Index\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#Visualize all 3 PLS results in the same plot\n",
    "plt.figure(figsize=(20, 14))\n",
    "plt.title(\"PLS Layer Output (All Materials)\")\n",
    "plt.plot(pes_pls.numpy()[0], label='PES')\n",
    "plt.plot(cotton_pls.numpy()[0], label='Cotton')\n",
    "plt.plot(wool_pls.numpy()[0], label='Wool')\n",
    "plt.xlabel(\"PLS Component Index\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c17232a-3981-4d90-908f-de1304caab1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing the SVD and its weights\n",
    "svd_layer = model.layers[0]\n",
    "\n",
    "# Plotting the SVD Matrix\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(svd_layer.svd_matrix.numpy(), aspect='auto', cmap='viridis')\n",
    "plt.title(\"PLS Matrix\")\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "# Plotting the SVD layer Bias\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(svd_layer.bias.numpy())\n",
    "plt.title(\"PLS Bias\")\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Bias Value\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089815f6-7c3c-42d0-a429-22533f772554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing the PLSLayer and its weights\n",
    "pls_layer = model.layers[1]  # The PLSLayer is the second layer in the model\n",
    "\n",
    "# Plotting the PLS Matrix (If non-trainable, displays the result of \n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(pls_layer.pls_matrix.numpy(), aspect='auto', cmap='viridis')\n",
    "plt.title(\"PLS Matrix\")\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "# Plotting the PLS Bias\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(pls_layer.bias.numpy())\n",
    "plt.title(\"PLS Bias\")\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Bias Value\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66fafe0-a9c2-4138-a906-0110c44f7f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analytics, Visualization, e.g.\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Confusion Matrix\n",
    "raw_cm = confusion_matrix(y_test_int, predicted_labels)\n",
    "normalized_cm = confusion_matrix(y_test_int, predicted_labels, normalize='all')\n",
    "\n",
    "labels = []\n",
    "for i in range(len(raw_cm)):\n",
    "    for j in range(len(raw_cm)):\n",
    "        labels.append(f\"{raw_cm[i, j]} ({normalized_cm[i, j]:.2f})\")\n",
    "labels = np.array(labels).reshape(raw_cm.shape)\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(normalized_cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.colorbar()\n",
    "\n",
    "# Adding annotations for raw counts and proportions\n",
    "for i in range(len(raw_cm)):\n",
    "    for j in range(len(raw_cm)):\n",
    "        # Annotate raw count and normalized value\n",
    "        text = f\"{raw_cm[i, j]}\\n({normalized_cm[i, j]:.2f})\"\n",
    "        plt.text(\n",
    "            j, i, text,\n",
    "            ha=\"center\", va=\"center\", fontsize=10,\n",
    "            bbox=dict(boxstyle=\"round\", facecolor='white', edgecolor='0.3')\n",
    "        )\n",
    "\n",
    "# Labels and title\n",
    "plt.title(\"Confusion Matrix (Counts and Proportions)\", fontsize=16)\n",
    "plt.xlabel(\"Predicted Label\", fontsize=12)\n",
    "plt.ylabel(\"True Label\", fontsize=12)\n",
    "plt.xticks(np.arange(len(raw_cm)), labels=np.arange(len(raw_cm)))\n",
    "plt.yticks(np.arange(len(raw_cm)), labels=np.arange(len(raw_cm)))\n",
    "plt.show()\n",
    "\n",
    "# ROC\n",
    "label_binarizer = LabelBinarizer()\n",
    "y_onehot_test = label_binarizer.fit_transform(y_test_int)\n",
    "\n",
    "for class_idx in range(len(label_binarizer.classes_)):\n",
    "  fpr, tpr, thresholds = roc_curve(y_onehot_test[:, class_idx], predictions[:, class_idx])\n",
    "  roc_auc = auc(fpr, tpr)\n",
    "  display = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc,\n",
    "                             estimator_name=f\"Class {label_binarizer.classes_[class_idx]}\")\n",
    "  display.plot(ax=plt.gca())\n",
    "plt.show()\n",
    "\n",
    "# Metrics\n",
    "for i in range(len(raw_cm)):\n",
    "    precision = precision_score(y_test_int == i, predicted_labels == i)\n",
    "    recall = recall_score(y_test_int == i, predicted_labels == i)\n",
    "    f1 = f1_score(y_test_int == i, predicted_labels == i)\n",
    "    print(f\"Metrics for Class {i}:\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1-Score: {f1}\")\n",
    "    print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254003ff-c4f5-4f1f-bb9e-9da622bbefb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example:\n",
    "# Requires validation folder to have samples\n",
    "def predict_textile(feature_list: list[float]):\n",
    "    features = np.expand_dims(feature_list, axis=0)\n",
    "    predictions = model.predict(features, verbose=0)\n",
    "    flattened = np.argmax(predictions, axis=1)\n",
    "    fabric_types = label_encoder.inverse_transform(flattened)\n",
    "    return predictions, fabric_types\n",
    "\n",
    "def run_validations():\n",
    "    # Import dataset\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    data_path = \"../data/\"\n",
    "    files = process_path(data_path)\n",
    "    for file_path in files:\n",
    "        file_label = parse_label(file_path)\n",
    "        file_features = parse_json_data(file_path)\n",
    "        all_labels.append(file_label)\n",
    "        all_features.append(file_features)\n",
    "\n",
    "    num_vals = len(all_features)\n",
    "    i = 0\n",
    "    pes_incorrect = 0\n",
    "    cotton_incorrect = 0\n",
    "    wool_incorrect = 0\n",
    "    while i < num_vals-1:\n",
    "        predictions, fabric_types = predict_textile(all_features[i])\n",
    "        print(f\"Validating {all_labels[i]} - Out: {fabric_types[0]}\\n\")\n",
    "        if not all_labels[i] == fabric_types[0]:\n",
    "            if all_labels[i] == \"pes\":\n",
    "                pes_incorrect += 1\n",
    "            elif all_labels[i] == \"puuvilla\":\n",
    "                cotton_incorrect += 1\n",
    "            elif all_labels[i] == \"villa\":\n",
    "                wool_incorrect += 1\n",
    "        i += 1\n",
    "\n",
    "    print(f\"Results:\\n Pes Incorrect: {pes_incorrect}\\n Cotton Incorrect: {cotton_incorrect}\\n Wool Incorrect: {wool_incorrect}\")\n",
    "    return\n",
    "\n",
    "# Uncomment to run model against all samples in the dataset\n",
    "# Results return the amount of incorrect categorial predicts\n",
    "#run_validations()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
