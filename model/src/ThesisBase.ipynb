{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917f1aae-7778-49e0-9098-67d0f44c32a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from scipy.signal import savgol_filter\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import LabelEncoder, LabelBinarizer\n",
    "from sklearn.metrics import r2_score, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc, RocCurveDisplay, classification_report\n",
    "from sklearn.utils import compute_class_weight\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import tensorflow_model_optimization as tfmot\n",
    "import tensorflow as tf\n",
    "import tempfile\n",
    "from tensorflow import keras\n",
    "from tensorflow_model_optimization.python.core.keras.compat import keras # Note: needed for quantization\n",
    "import joblib\n",
    "import pathlib\n",
    "import os\n",
    "from data_parser import parse_json_data, process_path, parse_label\n",
    "\n",
    "tf.config.run_functions_eagerly(True)\n",
    "tf.data.experimental.enable_debug_mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7f95a0-9b6e-4c55-81d9-349e15cd5802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset\n",
    "data_path = \"../data/\"\n",
    "window_size = 30\n",
    "poly_order = 3\n",
    "test_size_split = 0.20\n",
    "validation_size_split = 0.20\n",
    "\n",
    "all_features = []\n",
    "all_labels = []\n",
    "pes_features = []\n",
    "wool_features = []\n",
    "cotton_features = []\n",
    "\n",
    "#y_smooth = savgol_filter(pes.loc[0], window_size, poly_order)\n",
    "# Apply savgol while appending data features\n",
    "files = process_path(data_path)\n",
    "for file_path in files:\n",
    "    file_label = parse_label(file_path)\n",
    "    file_features = parse_json_data(file_path)\n",
    "    #file_features = savgol_filter(file_features, window_size, poly_order)\n",
    "    if (file_label == \"pes\"):\n",
    "        file_label = [1, 0, 0]\n",
    "        pes_features.append(file_features)\n",
    "    elif (file_label == \"puuvilla\"):\n",
    "        file_label = [0, 1, 0]\n",
    "        cotton_features.append(file_features)\n",
    "    elif (file_label == \"villa\"):\n",
    "        wool_features.append(file_features)\n",
    "        file_label = [0, 0, 1]\n",
    "    \n",
    "    all_labels.append(file_label)\n",
    "    all_features.append(file_features)\n",
    "# Now we continue with calculating splits so we must use the smallest sample count list for equal splits max\n",
    "if len(pes_features) > len(cotton_features) & len(pes_features) > len(wool_features):\n",
    "    max_size = len(pes_features)\n",
    "elif len(cotton_features) > len(pes_features) & len(cotton_features) > len(wool_features):\n",
    "    max_size = len(cotton_features)\n",
    "else:\n",
    "    max_size = len(wool_features)\n",
    "print(f\"Max Size for equal distribution {max_size}\")\n",
    "\n",
    "# Here we split dataset to equal distribution where rounded to nearest 10 scans / sample\n",
    "# and where length of the training set can be divisible to nearest 10 scans for test split\n",
    "feature_samples_count = max_size - (int)(max_size*validation_size_split)\n",
    "while feature_samples_count % 10 != 0:\n",
    "    feature_samples_count -= 1\n",
    "print(\"Nearest 10 scans count for all features after validation split\")\n",
    "print(feature_samples_count)\n",
    "\n",
    "# For train test split we must calculate nearest percentage where split results divisible by 10\n",
    "# By default we decrease test split size by 0.001 per search step\n",
    "all_eq_max = feature_samples_count * 3 # for 3 categories max amnt\n",
    "nearest_split_count = int(all_eq_max * test_size_split)\n",
    "print(f\"Current train-test split {int(nearest_split_count)} with {test_size_split}\")\n",
    "current_perc = test_size_split\n",
    "while nearest_split_count % 10 != 0:\n",
    "    if current_perc < 0.01: #end search\n",
    "        break\n",
    "    current_perc -= 0.001\n",
    "    nearest_split_count = int(all_eq_max * current_perc)\n",
    "# Now we can either use current_perc or nearest_split_count thats calculated for 10 divisble train splits\n",
    "# Note: Passing current_perc into train test split does NOT ALWAYS correspond to calculated nearest_split_count\n",
    "# maybe due to floating point rounding in their implementation.\n",
    "print(f\"Nearest test size: {nearest_split_count} samples - percentage of features {current_perc}\")\n",
    "test_size_split = current_perc\n",
    "\n",
    "# 10 divisible validation samples\n",
    "validation_samples_count = max_size - feature_samples_count\n",
    "while validation_samples_count % 10 != 0:\n",
    "    validation_samples_count -= 1\n",
    "print(\"Nearest 10 scans count for validation split\")\n",
    "print(validation_samples_count)\n",
    "\n",
    "pes = pd.DataFrame(pes_features[:feature_samples_count])\n",
    "cotton = pd.DataFrame(cotton_features[:feature_samples_count])\n",
    "wool = pd.DataFrame(wool_features[:feature_samples_count])\n",
    "# Combine train-test features\n",
    "all_eq_features = pd.concat([pes, cotton, wool], axis=0).reset_index(drop=True)\n",
    "\n",
    "last = feature_samples_count + validation_samples_count\n",
    "pes_val = pd.DataFrame(pes_features[feature_samples_count:last])\n",
    "cotton_val = pd.DataFrame(cotton_features[feature_samples_count:last])\n",
    "wool_val = pd.DataFrame(wool_features[feature_samples_count:last])\n",
    "# Combine validation dataset\n",
    "all_val_features = pd.concat([pes_val, cotton_val, wool_val], axis=0).reset_index(drop=True)\n",
    "\n",
    "# One-hot encode labels, this will be our output layer format for softmax true\n",
    "fabric_types = [\"pes\", \"puuvilla\", \"villa\"]\n",
    "label_array = np.array([[1, 0, 0]] * feature_samples_count \n",
    "                       + [[0, 1, 0]] * feature_samples_count \n",
    "                       + [[0, 0, 1]] * feature_samples_count\n",
    "                      )\n",
    "labels_eq = pd.DataFrame(label_array, columns=fabric_types)\n",
    "\n",
    "y_val_labels = np.array([[1, 0, 0]] * validation_samples_count \n",
    "                       + [[0, 1, 0]] * validation_samples_count\n",
    "                       + [[0, 0, 1]] * validation_samples_count\n",
    "                      )\n",
    "val_labels_eq = pd.DataFrame(y_val_labels, columns=fabric_types)\n",
    "val_labels_int = np.argmax(val_labels_eq, axis=1)\n",
    "\n",
    "# Full dataset in dataframes for conveniance\n",
    "features = pd.DataFrame(all_features)\n",
    "labels = pd.DataFrame(all_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a86c2b-74b4-4c2c-af03-77614aae4184",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_labels = labels_eq.idxmax(axis=1)\n",
    "\n",
    "# Split data to train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    all_eq_features, \n",
    "    labels_eq, \n",
    "    test_size=nearest_split_count,\n",
    "    random_state=42,\n",
    "    stratify=y_labels\n",
    ")\n",
    "\n",
    "y_train_int = np.argmax(y_train, axis=1)\n",
    "y_test_int = np.argmax(y_test, axis=1)\n",
    "\n",
    "def plot_class_distribution(y_data, label=\"Dataset\"):\n",
    "    class_counts = pd.Series(y_data).value_counts()\n",
    "    class_counts.plot(kind='bar')\n",
    "    plt.title(f\"Class Distribution in {label}\")\n",
    "    plt.xlabel(\"Class\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()\n",
    "\n",
    "# Convert one-hot encoded labels back to categorical labels\n",
    "y_train_labels = y_train.idxmax(axis=1)\n",
    "y_test_labels = y_test.idxmax(axis=1)\n",
    "# Plot for train and test sets\n",
    "print(len(y_train_labels))\n",
    "print(len(y_test_labels))\n",
    "plot_class_distribution(y_train_labels, \"Training Set\")\n",
    "plot_class_distribution(y_test_labels, \"Testing Set\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdf2f1d-2ab5-4b42-81e3-0b5bec72ab8f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define max total epochs\n",
    "total_epochs = 20\n",
    "\n",
    "# Build Model\n",
    "model = keras.models.Sequential([\n",
    "    keras.Input(shape=(512,)),\n",
    "    keras.layers.Dense(256, activation='relu'),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(32, activation='relu'),\n",
    "    keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Early stopping after loss function does not improve in 3 consecutive epochs\n",
    "callback = keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(\n",
    "    X_train, \n",
    "    y_train_int,\n",
    "    validation_data=(all_val_features, val_labels_int),\n",
    "    epochs=total_epochs,\n",
    "    callbacks=[callback],\n",
    "    batch_size=10, # Maybe the batch size isnt crucial here afterall, should research gradient optimization...\n",
    ")\n",
    "\n",
    "model.evaluate(X_test, y_test_int, verbose=0)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b6167b-38d2-44f2-8bcf-80ef68f2fa91",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_pred = model.predict(all_val_features)\n",
    "y_val_pred_classes = np.argmax(y_val_pred, axis=1)\n",
    "print(classification_report(val_labels_int, y_val_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f895f3e-3986-4553-9c95-27a5b3d2a0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Right click on cell after exec -> Disable scrolling for outputs\n",
    "\n",
    "# Helper function to make statistics and analytics from both Test and Validation datasets\n",
    "def analyze_model_performance(true_labels, predictions, predicted_labels, dataset_name):\n",
    "    print(f\"{dataset_name} Dataset\")\n",
    "\n",
    "    # Confusion Matrix\n",
    "    raw_cm = confusion_matrix(true_labels, predicted_labels)\n",
    "    normalized_cm = confusion_matrix(true_labels, predicted_labels, normalize='all')\n",
    "\n",
    "    labels = []\n",
    "    num_classes = raw_cm.shape[0]\n",
    "    for i in range(num_classes):\n",
    "        for j in range(num_classes):\n",
    "            labels.append(f\"{raw_cm[i, j]} ({normalized_cm[i, j]:.2f})\")\n",
    "    labels = np.array(labels).reshape(raw_cm.shape)\n",
    "\n",
    "    # Plotting the confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(normalized_cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.colorbar()\n",
    "\n",
    "    # Adding annotations for raw counts and proportions\n",
    "    for i in range(num_classes):\n",
    "        for j in range(num_classes):\n",
    "            # Annotate raw count and normalized value\n",
    "            text = f\"{raw_cm[i, j]}\\n({normalized_cm[i, j]:.2f})\"\n",
    "            plt.text(\n",
    "                j, i, text,\n",
    "                ha=\"center\", va=\"center\", fontsize=10,\n",
    "                bbox=dict(boxstyle=\"round\", facecolor='white', edgecolor='0.3')\n",
    "            )\n",
    "\n",
    "    # Labels and title\n",
    "    plt.title(f\"Confusion Matrix Against {dataset_name}\", fontsize=16)\n",
    "    plt.xlabel(\"Predicted Label\", fontsize=12)\n",
    "    plt.ylabel(\"True Label\", fontsize=12)\n",
    "    plt.xticks(np.arange(num_classes), labels=np.arange(num_classes))\n",
    "    plt.yticks(np.arange(num_classes), labels=np.arange(num_classes))\n",
    "    plt.show()\n",
    "\n",
    "    # ROC\n",
    "    label_binarizer = LabelBinarizer()\n",
    "    all_possible_labels = np.unique(np.concatenate((true_labels, predicted_labels)))\n",
    "    label_binarizer.fit(all_possible_labels)\n",
    "\n",
    "    y_onehot = label_binarizer.transform(true_labels)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    # Iterate through the classes the binarizer was fitted on\n",
    "    for class_idx, class_label in enumerate(label_binarizer.classes_):\n",
    "        # Check if the class is present in the true labels for this dataset\n",
    "        if class_label in all_possible_labels:\n",
    "            fpr, tpr, thresholds = roc_curve(y_onehot[:, class_idx], predictions[:, class_idx])\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            display = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc,\n",
    "                                      estimator_name=f\"Class {class_label}\")\n",
    "            display.plot(ax=plt.gca())\n",
    "    plt.title(f\"ROC Curves for {dataset_name} Dataset\", fontsize=16)\n",
    "    plt.xlabel(\"False Positive Rate\", fontsize=12)\n",
    "    plt.ylabel(\"True Positive Rate\", fontsize=12)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # Metrics\n",
    "    print(f\"Metrics for {dataset_name} Dataset:\")\n",
    "    # Calculate metrics for each class present in the true labels\n",
    "    for i in np.unique(true_labels):\n",
    "        # Use true_labels == i and predicted_labels == i for binary classification per class\n",
    "        precision = precision_score(true_labels == i, predicted_labels == i, zero_division=0)\n",
    "        recall = recall_score(true_labels == i, predicted_labels == i, zero_division=0)\n",
    "        f1 = f1_score(true_labels == i, predicted_labels == i, zero_division=0)\n",
    "        print(f\"Metrics for Class {i}:\")\n",
    "        print(f\"  Precision: {precision:.4f}\")\n",
    "        print(f\"  Recall: {recall:.4f}\")\n",
    "        print(f\"  F1-Score: {f1:.4f}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "# Analytics against Test dataset\n",
    "test_predictions = model.predict(X_test)\n",
    "test_predicted_labels_keras = np.argmax(test_predictions, axis=1)\n",
    "analyze_model_performance(y_test_int, test_predictions, test_predicted_labels_keras, \"Test\")\n",
    "\n",
    "# Analytics against Validation dataset\n",
    "val_predictions = model.predict(all_val_features)\n",
    "val_predicted_labels_keras = np.argmax(val_predictions, axis=1)\n",
    "analyze_model_performance(val_labels_int, val_predictions, val_predicted_labels_keras, \"Validation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1aedac5-aa7a-4a0c-afc5-93a5afc48b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source 1: https://www.tensorflow.org/model_optimization/guide/quantization/training_example\n",
    "# Source 2: https://www.tensorflow.org/model_optimization/guide/quantization/post_training\n",
    "\n",
    "# Now we quantize the model weights to reduce its size and prepare it for TFLite conversion\n",
    "quantize_model = tfmot.quantization.keras.quantize_model\n",
    "q_aware_model = quantize_model(model)\n",
    "\n",
    "# `quantize_model` requires a recompile.\n",
    "q_aware_model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# q_aware_model.summary()\n",
    "\n",
    "# After quantization we must fit to regain accuracy\n",
    "# Optional approach to this is to quantize model pre-training but it is kept like this to display accuracy difference\n",
    "q_aware_model.fit(X_train, y_train_int, batch_size=10, epochs=1, validation_data=(all_val_features, val_labels_int), verbose=0)\n",
    "\n",
    "_, baseline_model_accuracy = model.evaluate(\n",
    "    X_test, y_test_int, verbose=0)\n",
    "\n",
    "_, q_aware_model_accuracy = q_aware_model.evaluate(\n",
    "   X_test, y_test_int, verbose=0)\n",
    "\n",
    "print('Baseline test accuracy:', baseline_model_accuracy)\n",
    "print('Quant test accuracy:', q_aware_model_accuracy)\n",
    "\n",
    "# Create a TFLite model from the quantization aware model that has been fine-tuned\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\n",
    "# According to Tensorflow, 8-bit integer weights are recommended for CPU Execution which is our case\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "quantized_tflite_model = converter.convert()\n",
    "\n",
    "# Helper function for getting accuracy of the TFLite model against X_test\n",
    "def evaluate_model(interpreter):\n",
    "    input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "    output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "\n",
    "    # Run predictions on every measurement in the \"test\" dataset.\n",
    "    prediction_digits = []\n",
    "    for i in range(len(X_test)):\n",
    "        test_data = X_test.iloc[i].values\n",
    "        test_data = np.expand_dims(test_data, axis=0).astype(np.float32)\n",
    "        interpreter.set_tensor(input_index, test_data)\n",
    "        interpreter.invoke()\n",
    "        \n",
    "        output = interpreter.tensor(output_index)\n",
    "        digit = np.argmax(output()[0])\n",
    "        prediction_digits.append(digit)\n",
    "\n",
    "    # Compare prediction results with ground truth labels to calculate accuracy.\n",
    "    accuracy = (prediction_digits == y_test_int).mean()\n",
    "    return accuracy\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_content=quantized_tflite_model)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "test_accuracy = evaluate_model(interpreter)\n",
    "\n",
    "print('Quant TFLite test_accuracy:', test_accuracy)\n",
    "print('Quant TF test accuracy:', q_aware_model_accuracy)\n",
    "\n",
    "# Create TFLite model from the original non-quantized model for comparison\n",
    "float_converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "float_tflite_model = float_converter.convert()\n",
    "\n",
    "# Measure sizes of models.\n",
    "_, float_file = tempfile.mkstemp('.tflite')\n",
    "_, quant_file = tempfile.mkstemp('.tflite')\n",
    "\n",
    "with open(quant_file, 'wb') as f:\n",
    "  f.write(quantized_tflite_model)\n",
    "\n",
    "with open(float_file, 'wb') as f:\n",
    "  f.write(float_tflite_model)\n",
    "\n",
    "print(\"Float model in Mb:\", os.path.getsize(float_file) / float(2**20))\n",
    "print(\"Quantized model in Mb:\", os.path.getsize(quant_file) / float(2**20))\n",
    "\n",
    "# Save the quantized tflite model into models dir\n",
    "quant_model_path = \"../models/model.tflite\"\n",
    "# Write quantized model to file\n",
    "with open(quant_model_path, 'wb') as f:\n",
    "    f.write(quantized_tflite_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254003ff-c4f5-4f1f-bb9e-9da622bbefb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example:\n",
    "def predict_textile(feature_list: list[float]):\n",
    "    # Encode fabric_type labels with integers\n",
    "    label_encoder = LabelEncoder()\n",
    "    labels['fabric_type_encoded'] = label_encoder.fit_transform(labels['fabric_type'])\n",
    "    # Prepare features\n",
    "    features = np.expand_dims(feature_list, axis=0)\n",
    "    predictions = model.predict(features, verbose=0)\n",
    "    flattened = np.argmax(predictions, axis=1)\n",
    "    fabric_types = label_encoder.inverse_transform(flattened)\n",
    "    return predictions, fabric_types\n",
    "\n",
    "def run_validations():\n",
    "    # Import dataset\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    data_path = \"../data/\"\n",
    "    files = process_path(data_path)\n",
    "    for file_path in files:\n",
    "        file_label = parse_label(file_path)\n",
    "        file_features = parse_json_data(file_path)\n",
    "        all_labels.append(file_label)\n",
    "        all_features.append(file_features)\n",
    "\n",
    "    num_vals = len(all_features)\n",
    "    i = 0\n",
    "    pes_incorrect = 0\n",
    "    cotton_incorrect = 0\n",
    "    wool_incorrect = 0\n",
    "    while i < num_vals-1:\n",
    "        predictions, fabric_types = predict_textile(all_features[i])\n",
    "        print(f\"Validating {all_labels[i]} - Out: {fabric_types[0]}\\n\")\n",
    "        if not all_labels[i] == fabric_types[0]:\n",
    "            if all_labels[i] == \"pes\":\n",
    "                pes_incorrect += 1\n",
    "            elif all_labels[i] == \"puuvilla\":\n",
    "                cotton_incorrect += 1\n",
    "            elif all_labels[i] == \"villa\":\n",
    "                wool_incorrect += 1\n",
    "        i += 1\n",
    "\n",
    "    print(f\"Results:\\n Pes Incorrect: {pes_incorrect}\\n Cotton Incorrect: {cotton_incorrect}\\n Wool Incorrect: {wool_incorrect}\")\n",
    "    return\n",
    "\n",
    "# Uncomment to run model against all samples in the dataset\n",
    "# Results return the amount of incorrect categorial predicts\n",
    "# run_validations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57152c82-d5d4-45e0-ae9c-f507595f331c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
